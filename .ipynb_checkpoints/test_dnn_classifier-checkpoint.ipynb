{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test DNN Classifier\n",
    "\n",
    "This classifier tests classification of an embedding layer feeding into a convolution layer on the news20 data set.\n",
    "The details of the data set and the performance of other classifiers is described here:\n",
    "https://nlp.stanford.edu/wiki/Software/Classifier/20_Newsgroups\n",
    "\n",
    "The aim of this classifier is to attempt to benchmark above 80% on the test set or better.\n",
    "The news 20 data set has a large vocabulary and reasonably long utterances. \n",
    "On different domains with smaller restricted vocabularies I've found that the approach performs quite well, this is because of the limited size of the vocabulary.\n",
    "Hence the news 20 dataset provides a good level of complexity to test against for the multi-class text classification problem on short passages of text.\n",
    "\n",
    "This page provides a good exploration of the data set and a survey of a variety of different resources testing a variety of different classification algorithms against it.\n",
    "https://acardocacho.github.io/capstone/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sourcing the Data.\n",
    "\n",
    "This project makes use of the glove embeddings.\n",
    "Download the glove embeddings from this link: \n",
    "- https://nlp.stanford.edu/data/glove.6B.zip\n",
    "\n",
    "Create a directory data/glove/\n",
    "\n",
    "And extract the contents below that directory this should provide the listing glove/glove.6N.50d.txt for example.\n",
    "\n",
    "\n",
    "Obtain the news20 data set, this has been processed and generously made available here: http://ana.cachopo.org/datasets-for-single-label-text-categorization\n",
    "Download the training set and test sets:\n",
    "- http://ana.cachopo.org/datasets-for-single-label-text-categorization/20ng-train-all-terms.txt?attredirects=0\n",
    "and\n",
    "- http://ana.cachopo.org/datasets-for-single-label-text-categorization/20ng-test-all-terms.txt?attredirects=0\n",
    "\n",
    "Place the text files in the path data/news20/ so the listing will be:\n",
    "\n",
    "- data/news20/20ng-train-all-terms.txt\n",
    "- data/news20/20ng-test-all-terms.txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First steps prepare the data for use with the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "sys.path.insert(0, 'lib')\n",
    "\n",
    "from lib import TextReader\n",
    "from lib import GloveReader\n",
    "\n",
    "\n",
    "basedir = 'data'\n",
    "\n",
    "\n",
    "data_path = os.path.join(basedir, 'news20')\n",
    "data_path = os.path.join(data_path, 'train_data_all.pickle')\n",
    "vocab = []\n",
    "all_words = []\n",
    "all_classes = []\n",
    "targets = None\n",
    "sequences = None\n",
    "reader = TextReader.TextReader(os.path.join(basedir, 'news20'), basedir)\n",
    "    \n",
    "if os.path.exists(data_path):\n",
    "    with open(data_path, 'rb') as fin:\n",
    "        all_data = pickle.load(fin)\n",
    "        vocab = all_data['vocab']\n",
    "        all_words = all_data['all_words']\n",
    "        all_classes = all_data['all_classes']\n",
    "        targets = all_data['targets']\n",
    "        sequences = all_data['sequences']\n",
    "else:\n",
    "    vocab, all_words, all_classes = reader.read_labeled_documents('20ng-train-all-terms.txt')\n",
    "    targets = reader.one_hot_encode_classes(all_classes)\n",
    "    sequences = reader.make_index_sequences(vocab, all_words)\n",
    "    all_data = {\n",
    "        'vocab':vocab,\n",
    "        'all_words':all_words,\n",
    "        'all_classes': all_classes,\n",
    "        'targets': targets,\n",
    "        'sequences': sequences\n",
    "    }\n",
    "    with open(data_path, 'wb') as fout:\n",
    "        pickle.dump(all_data, fout)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73404"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>6967</th>\n",
       "      <th>6968</th>\n",
       "      <th>6969</th>\n",
       "      <th>6970</th>\n",
       "      <th>6971</th>\n",
       "      <th>6972</th>\n",
       "      <th>6973</th>\n",
       "      <th>6974</th>\n",
       "      <th>6975</th>\n",
       "      <th>6976</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>56323</td>\n",
       "      <td>28135</td>\n",
       "      <td>57319</td>\n",
       "      <td>4092</td>\n",
       "      <td>30915</td>\n",
       "      <td>38391</td>\n",
       "      <td>53721</td>\n",
       "      <td>39186</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>39186</td>\n",
       "      <td>31216</td>\n",
       "      <td>47898</td>\n",
       "      <td>57319</td>\n",
       "      <td>38391</td>\n",
       "      <td>47899</td>\n",
       "      <td>38741</td>\n",
       "      <td>66716</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>5337</td>\n",
       "      <td>38897</td>\n",
       "      <td>49969</td>\n",
       "      <td>70135</td>\n",
       "      <td>12616</td>\n",
       "      <td>52881</td>\n",
       "      <td>33507</td>\n",
       "      <td>5992</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 6977 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4     5     6     7     8     9     ...    6967  \\\n",
       "0     1     1     1     1     1     1     1     1     1     1  ...   56323   \n",
       "0     1     1     1     1     1     1     1     1     1     1  ...   39186   \n",
       "0     1     1     1     1     1     1     1     1     1     1  ...    5337   \n",
       "\n",
       "    6968   6969   6970   6971   6972   6973   6974  6975  6976  \n",
       "0  28135  57319   4092  30915  38391  53721  39186     0     3  \n",
       "0  31216  47898  57319  38391  47899  38741  66716     0     3  \n",
       "0  38897  49969  70135  12616  52881  33507   5992     0     3  \n",
       "\n",
       "[3 rows x 6977 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alt.atheism</th>\n",
       "      <th>comp.graphics</th>\n",
       "      <th>comp.os.ms-windows.misc</th>\n",
       "      <th>comp.sys.ibm.pc.hardware</th>\n",
       "      <th>comp.sys.mac.hardware</th>\n",
       "      <th>comp.windows.x</th>\n",
       "      <th>misc.forsale</th>\n",
       "      <th>rec.autos</th>\n",
       "      <th>rec.motorcycles</th>\n",
       "      <th>rec.sport.baseball</th>\n",
       "      <th>rec.sport.hockey</th>\n",
       "      <th>sci.crypt</th>\n",
       "      <th>sci.electronics</th>\n",
       "      <th>sci.med</th>\n",
       "      <th>sci.space</th>\n",
       "      <th>soc.religion.christian</th>\n",
       "      <th>talk.politics.guns</th>\n",
       "      <th>talk.politics.mideast</th>\n",
       "      <th>talk.politics.misc</th>\n",
       "      <th>talk.religion.misc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alt.atheism  comp.graphics  comp.os.ms-windows.misc  \\\n",
       "0            1              0                        0   \n",
       "1            1              0                        0   \n",
       "2            1              0                        0   \n",
       "\n",
       "   comp.sys.ibm.pc.hardware  comp.sys.mac.hardware  comp.windows.x  \\\n",
       "0                         0                      0               0   \n",
       "1                         0                      0               0   \n",
       "2                         0                      0               0   \n",
       "\n",
       "   misc.forsale  rec.autos  rec.motorcycles  rec.sport.baseball  \\\n",
       "0             0          0                0                   0   \n",
       "1             0          0                0                   0   \n",
       "2             0          0                0                   0   \n",
       "\n",
       "   rec.sport.hockey  sci.crypt  sci.electronics  sci.med  sci.space  \\\n",
       "0                 0          0                0        0          0   \n",
       "1                 0          0                0        0          0   \n",
       "2                 0          0                0        0          0   \n",
       "\n",
       "   soc.religion.christian  talk.politics.guns  talk.politics.mideast  \\\n",
       "0                       0                   0                      0   \n",
       "1                       0                   0                      0   \n",
       "2                       0                   0                      0   \n",
       "\n",
       "   talk.politics.misc  talk.religion.misc  \n",
       "0                   0                   0  \n",
       "1                   0                   0  \n",
       "2                   0                   0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we preload the glove embeddings and create an embedding matrix for our training vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 18137: Expected 51 fields in line 18137, saw 52\n",
      "Skipping line 77306: Expected 51 fields in line 77306, saw 52\n",
      "Skipping line 78481: Expected 51 fields in line 78481, saw 52\n",
      "Skipping line 80636: Expected 51 fields in line 80636, saw 52\n",
      "Skipping line 86603: Expected 51 fields in line 86603, saw 52\n",
      "Skipping line 95766: Expected 51 fields in line 95766, saw 52\n",
      "Skipping line 97253: Expected 51 fields in line 97253, saw 52\n",
      "Skipping line 98622: Expected 51 fields in line 98622, saw 52\n",
      "Skipping line 102606: Expected 51 fields in line 102606, saw 52\n",
      "Skipping line 104608: Expected 51 fields in line 104608, saw 52\n",
      "Skipping line 120311: Expected 51 fields in line 120311, saw 52\n",
      "Skipping line 123556: Expected 51 fields in line 123556, saw 52\n",
      "Skipping line 129697: Expected 51 fields in line 129697, saw 52\n",
      "Skipping line 140365: Expected 51 fields in line 140365, saw 52\n",
      "Skipping line 141336: Expected 51 fields in line 141336, saw 52\n",
      "Skipping line 147469: Expected 51 fields in line 147469, saw 52\n",
      "Skipping line 165670: Expected 51 fields in line 165670, saw 52\n",
      "Skipping line 166086: Expected 51 fields in line 166086, saw 52\n",
      "Skipping line 168260: Expected 51 fields in line 168260, saw 52\n",
      "Skipping line 168985: Expected 51 fields in line 168985, saw 52\n",
      "Skipping line 173305: Expected 51 fields in line 173305, saw 52\n",
      "Skipping line 173425: Expected 51 fields in line 173425, saw 52\n",
      "Skipping line 181649: Expected 51 fields in line 181649, saw 52\n",
      "Skipping line 182625: Expected 51 fields in line 182625, saw 52\n",
      "Skipping line 189110: Expected 51 fields in line 189110, saw 52\n",
      "Skipping line 190464: Expected 51 fields in line 190464, saw 52\n",
      "Skipping line 190787: Expected 51 fields in line 190787, saw 52\n",
      "Skipping line 192092: Expected 51 fields in line 192092, saw 52\n",
      "Skipping line 196216: Expected 51 fields in line 196216, saw 52\n",
      "Skipping line 197618: Expected 51 fields in line 197618, saw 52\n",
      "Skipping line 199295: Expected 51 fields in line 199295, saw 52\n",
      "Skipping line 199378: Expected 51 fields in line 199378, saw 52\n",
      "Skipping line 204816: Expected 51 fields in line 204816, saw 52\n",
      "Skipping line 205753: Expected 51 fields in line 205753, saw 52\n",
      "Skipping line 213265: Expected 51 fields in line 213265, saw 52\n",
      "Skipping line 217331: Expected 51 fields in line 217331, saw 52\n",
      "Skipping line 220405: Expected 51 fields in line 220405, saw 52\n",
      "Skipping line 221617: Expected 51 fields in line 221617, saw 52\n",
      "Skipping line 224038: Expected 51 fields in line 224038, saw 52\n",
      "Skipping line 229898: Expected 51 fields in line 229898, saw 52\n",
      "Skipping line 233116: Expected 51 fields in line 233116, saw 52\n",
      "Skipping line 237671: Expected 51 fields in line 237671, saw 52\n",
      "Skipping line 239936: Expected 51 fields in line 239936, saw 52\n",
      "Skipping line 242877: Expected 51 fields in line 242877, saw 52\n",
      "Skipping line 243556: Expected 51 fields in line 243556, saw 52\n",
      "Skipping line 249095: Expected 51 fields in line 249095, saw 52\n",
      "Skipping line 249701: Expected 51 fields in line 249701, saw 52\n",
      "Skipping line 252349: Expected 51 fields in line 252349, saw 52\n",
      "Skipping line 253427: Expected 51 fields in line 253427, saw 52\n",
      "Skipping line 253495: Expected 51 fields in line 253495, saw 52\n",
      "Skipping line 254067: Expected 51 fields in line 254067, saw 52\n",
      "Skipping line 257593: Expected 51 fields in line 257593, saw 52\n",
      "Skipping line 263696: Expected 51 fields in line 263696, saw 52\n",
      "Skipping line 266329: Expected 51 fields in line 266329, saw 52\n",
      "Skipping line 267299: Expected 51 fields in line 267299, saw 52\n",
      "Skipping line 272005: Expected 51 fields in line 272005, saw 52\n",
      "Skipping line 273324: Expected 51 fields in line 273324, saw 52\n",
      "Skipping line 274688: Expected 51 fields in line 274688, saw 52\n",
      "Skipping line 276429: Expected 51 fields in line 276429, saw 52\n",
      "Skipping line 280017: Expected 51 fields in line 280017, saw 52\n",
      "Skipping line 281756: Expected 51 fields in line 281756, saw 52\n",
      "Skipping line 282135: Expected 51 fields in line 282135, saw 52\n",
      "Skipping line 283012: Expected 51 fields in line 283012, saw 52\n",
      "Skipping line 283383: Expected 51 fields in line 283383, saw 52\n",
      "Skipping line 284088: Expected 51 fields in line 284088, saw 52\n",
      "Skipping line 288163: Expected 51 fields in line 288163, saw 52\n",
      "Skipping line 289997: Expected 51 fields in line 289997, saw 52\n",
      "Skipping line 292824: Expected 51 fields in line 292824, saw 52\n",
      "Skipping line 296060: Expected 51 fields in line 296060, saw 52\n",
      "Skipping line 296293: Expected 51 fields in line 296293, saw 52\n",
      "Skipping line 299409: Expected 51 fields in line 299409, saw 52\n",
      "Skipping line 312253: Expected 51 fields in line 312253, saw 52\n",
      "Skipping line 318233: Expected 51 fields in line 318233, saw 52\n",
      "Skipping line 319128: Expected 51 fields in line 319128, saw 52\n",
      "Skipping line 319951: Expected 51 fields in line 319951, saw 52\n",
      "Skipping line 321581: Expected 51 fields in line 321581, saw 52\n",
      "Skipping line 325387: Expected 51 fields in line 325387, saw 52\n",
      "Skipping line 329082: Expected 51 fields in line 329082, saw 52\n",
      "Skipping line 331435: Expected 51 fields in line 331435, saw 52\n",
      "Skipping line 332262: Expected 51 fields in line 332262, saw 52\n",
      "Skipping line 332972: Expected 51 fields in line 332972, saw 52\n",
      "Skipping line 336742: Expected 51 fields in line 336742, saw 52\n",
      "Skipping line 339098: Expected 51 fields in line 339098, saw 52\n",
      "Skipping line 346056: Expected 51 fields in line 346056, saw 52\n",
      "Skipping line 355456: Expected 51 fields in line 355456, saw 52\n",
      "Skipping line 362413: Expected 51 fields in line 362413, saw 52\n",
      "Skipping line 363235: Expected 51 fields in line 363235, saw 52\n",
      "Skipping line 370053: Expected 51 fields in line 370053, saw 52\n",
      "Skipping line 375057: Expected 51 fields in line 375057, saw 52\n",
      "Skipping line 380085: Expected 51 fields in line 380085, saw 52\n",
      "Skipping line 381654: Expected 51 fields in line 381654, saw 52\n",
      "Skipping line 382256: Expected 51 fields in line 382256, saw 52\n",
      "Skipping line 384937: Expected 51 fields in line 384937, saw 52\n",
      "Skipping line 388333: Expected 51 fields in line 388333, saw 52\n",
      "Skipping line 389951: Expected 51 fields in line 389951, saw 52\n",
      "Skipping line 390791: Expected 51 fields in line 390791, saw 52\n",
      "Skipping line 390796: Expected 51 fields in line 390796, saw 52\n",
      "Skipping line 393628: Expected 51 fields in line 393628, saw 52\n",
      "Skipping line 394745: Expected 51 fields in line 394745, saw 52\n",
      "Skipping line 397743: Expected 51 fields in line 397743, saw 52\n",
      "Skipping line 397956: Expected 51 fields in line 397956, saw 52\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embed_reader = GloveReader.GloveReader(base_dir=basedir)\n",
    "glove1 = embed_reader.read_glove_model('model50')\n",
    "\n",
    "\n",
    "# save the vocab embeddings because they are expensive to recreate.\n",
    "import pickle\n",
    "output = os.path.join(basedir, 'news20')\n",
    "output = os.path.join(output, 'vocab_embeddings.pickle')\n",
    "\n",
    "vocab_embedding = None\n",
    "if os.path.exists(output):\n",
    "    with open(output, 'rb') as fin:\n",
    "        vocab_embedding = pickle.load(fin)\n",
    "else:\n",
    "    vocab_embedding = reader.vocab_to_embedding_matrix(embed_reader, vocab)\n",
    "    with open(output, 'wb') as fout:\n",
    "        pickle.dump(vocab_embedding, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build the network and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# the vocab embedding can be used with our cnn embedding model.\n",
    "from lib import CnnClassifier\n",
    "\n",
    "classifier = CnnClassifier.CnnClassifier()\n",
    "\n",
    "max_sequence_length = sequences.shape[1]\n",
    "embed_dim = vocab_embedding.shape[1]\n",
    "num_outputs = targets.shape[1]\n",
    "pool_size = targets.shape[1]\n",
    "kernel_shape = 3\n",
    "dropout_pc = 0.3\n",
    "# note that filters correspond roughly with n-grams\n",
    "num_filters=21\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "(6977, 50, 1)\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 6977, 50)          3670200   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 6975, 21)          3171      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 6975, 21)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 6973, 21)          1344      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6973, 21)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 6971, 21)          1344      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6971, 21)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 348, 21)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 7308)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                146180    \n",
      "=================================================================\n",
      "Total params: 3,822,239\n",
      "Trainable params: 3,822,239\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model without training the embedding layer, train_embedding=False\n",
    "model = classifier.build_network(len(vocab), max_sequence_length, num_outputs, pool_size, kernel_shape, embed_dim, num_filters=num_filters, embedding_matrix=vocab_embedding, train_embedding=True)\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='nadam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rows = sequences.shape[0]\n",
    "# will shuffle the data\n",
    "indices = np.arange(rows)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "shuffled_inputs = sequences.values[indices]\n",
    "shuffled_targets = targets.values[indices]\n",
    "train_percent = 0.8\n",
    "trainX, validateX = np.split(shuffled_inputs, [int(train_percent*shuffled_inputs.shape[0])])\n",
    "trainY, validateY = np.split(shuffled_targets, [int(train_percent*shuffled_targets.shape[0])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model. Note prior to training run tensorboard from the base directory to monitor progress.\n",
    "\n",
    "```\n",
    "tensorboard --logdir logs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import keras\n",
    "\n",
    "logdir=os.path.join(\"logs\", \"scalars\")\n",
    "logdir=os.path.join(logdir, \"model1\")\n",
    "logdir=os.path.join(logdir, datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "epochs=100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 9034 samples, validate on 2259 samples\n",
      "Epoch 1/50\n",
      "9034/9034 [==============================] - 19s 2ms/step - loss: 3.1033 - categorical_accuracy: 0.1691 - val_loss: 2.1564 - val_categorical_accuracy: 0.2961\n",
      "Epoch 2/50\n",
      "9034/9034 [==============================] - 18s 2ms/step - loss: 1.8091 - categorical_accuracy: 0.4003 - val_loss: 1.5984 - val_categorical_accuracy: 0.4962\n",
      "Epoch 3/50\n",
      "9034/9034 [==============================] - 16s 2ms/step - loss: 1.3229 - categorical_accuracy: 0.5570 - val_loss: 1.3457 - val_categorical_accuracy: 0.5759\n",
      "Epoch 4/50\n",
      "9034/9034 [==============================] - 16s 2ms/step - loss: 0.9764 - categorical_accuracy: 0.6776 - val_loss: 1.1321 - val_categorical_accuracy: 0.6662\n",
      "Epoch 5/50\n",
      "9034/9034 [==============================] - 16s 2ms/step - loss: 0.7706 - categorical_accuracy: 0.7598 - val_loss: 1.0605 - val_categorical_accuracy: 0.6817\n",
      "Epoch 6/50\n",
      "9034/9034 [==============================] - 17s 2ms/step - loss: 0.6449 - categorical_accuracy: 0.8072 - val_loss: 0.9971 - val_categorical_accuracy: 0.7025\n",
      "Epoch 7/50\n",
      "9034/9034 [==============================] - 16s 2ms/step - loss: 0.4915 - categorical_accuracy: 0.8553 - val_loss: 0.9495 - val_categorical_accuracy: 0.7233\n",
      "Epoch 8/50\n",
      "9034/9034 [==============================] - 16s 2ms/step - loss: 0.3687 - categorical_accuracy: 0.8896 - val_loss: 0.9929 - val_categorical_accuracy: 0.7291\n",
      "Epoch 9/50\n",
      "9034/9034 [==============================] - 15s 2ms/step - loss: 0.3296 - categorical_accuracy: 0.9070 - val_loss: 0.9374 - val_categorical_accuracy: 0.7574\n",
      "Epoch 10/50\n",
      "9034/9034 [==============================] - 16s 2ms/step - loss: 0.2920 - categorical_accuracy: 0.9192 - val_loss: 0.9457 - val_categorical_accuracy: 0.7596\n",
      "Epoch 11/50\n",
      "9034/9034 [==============================] - 16s 2ms/step - loss: 0.2537 - categorical_accuracy: 0.9306 - val_loss: 0.9837 - val_categorical_accuracy: 0.7641\n",
      "Epoch 12/50\n",
      "9034/9034 [==============================] - 17s 2ms/step - loss: 0.2593 - categorical_accuracy: 0.9379 - val_loss: 1.0496 - val_categorical_accuracy: 0.7596\n",
      "Epoch 13/50\n",
      "9034/9034 [==============================] - 16s 2ms/step - loss: 0.2198 - categorical_accuracy: 0.9417 - val_loss: 0.9810 - val_categorical_accuracy: 0.7703\n",
      "Epoch 14/50\n",
      "9034/9034 [==============================] - 16s 2ms/step - loss: 0.1886 - categorical_accuracy: 0.9531 - val_loss: 1.0006 - val_categorical_accuracy: 0.7769\n",
      "Epoch 15/50\n",
      "9034/9034 [==============================] - 16s 2ms/step - loss: 0.1713 - categorical_accuracy: 0.9580 - val_loss: 1.1267 - val_categorical_accuracy: 0.7596\n",
      "Epoch 16/50\n",
      "9034/9034 [==============================] - 15s 2ms/step - loss: 0.1697 - categorical_accuracy: 0.9584 - val_loss: 1.0981 - val_categorical_accuracy: 0.7782\n",
      "Epoch 17/50\n",
      "9034/9034 [==============================] - 16s 2ms/step - loss: 0.1579 - categorical_accuracy: 0.9613 - val_loss: 1.1072 - val_categorical_accuracy: 0.7694\n",
      "Epoch 18/50\n",
      "9034/9034 [==============================] - 16s 2ms/step - loss: 0.1611 - categorical_accuracy: 0.9595 - val_loss: 1.1616 - val_categorical_accuracy: 0.7698\n",
      "Epoch 19/50\n",
      "9034/9034 [==============================] - 16s 2ms/step - loss: 0.1420 - categorical_accuracy: 0.9640 - val_loss: 1.3119 - val_categorical_accuracy: 0.7689\n",
      "Epoch 20/50\n",
      "9034/9034 [==============================] - 16s 2ms/step - loss: 0.2122 - categorical_accuracy: 0.9607 - val_loss: 1.5777 - val_categorical_accuracy: 0.7357\n",
      "Epoch 21/50\n",
      "9034/9034 [==============================] - 16s 2ms/step - loss: 0.5249 - categorical_accuracy: 0.9364 - val_loss: 1.3244 - val_categorical_accuracy: 0.7689\n",
      "Epoch 22/50\n",
      "9034/9034 [==============================] - 16s 2ms/step - loss: 0.1898 - categorical_accuracy: 0.9580 - val_loss: 1.1874 - val_categorical_accuracy: 0.7782\n",
      "Epoch 23/50\n",
      "9034/9034 [==============================] - 16s 2ms/step - loss: 0.1787 - categorical_accuracy: 0.9629 - val_loss: 1.2459 - val_categorical_accuracy: 0.7778\n",
      "Epoch 24/50\n",
      "9034/9034 [==============================] - 16s 2ms/step - loss: 0.1590 - categorical_accuracy: 0.9656 - val_loss: 1.3739 - val_categorical_accuracy: 0.7583\n",
      "Epoch 25/50\n",
      "9034/9034 [==============================] - 18s 2ms/step - loss: 0.2081 - categorical_accuracy: 0.9586 - val_loss: 1.1626 - val_categorical_accuracy: 0.7787\n",
      "Epoch 26/50\n",
      "9034/9034 [==============================] - 14s 2ms/step - loss: 0.1515 - categorical_accuracy: 0.9671 - val_loss: 1.0930 - val_categorical_accuracy: 0.7884\n",
      "Epoch 27/50\n",
      "9034/9034 [==============================] - 12s 1ms/step - loss: 0.1336 - categorical_accuracy: 0.9734 - val_loss: 1.1877 - val_categorical_accuracy: 0.7857\n",
      "Epoch 28/50\n",
      "9034/9034 [==============================] - 17s 2ms/step - loss: 0.1223 - categorical_accuracy: 0.9739 - val_loss: 1.1929 - val_categorical_accuracy: 0.7911\n",
      "Epoch 29/50\n",
      "9034/9034 [==============================] - 13s 1ms/step - loss: 0.1166 - categorical_accuracy: 0.9739 - val_loss: 1.2909 - val_categorical_accuracy: 0.7773\n",
      "Epoch 30/50\n",
      "9034/9034 [==============================] - 16s 2ms/step - loss: 0.1116 - categorical_accuracy: 0.9754 - val_loss: 1.1809 - val_categorical_accuracy: 0.7955\n",
      "Epoch 31/50\n",
      "9034/9034 [==============================] - 16s 2ms/step - loss: 0.1242 - categorical_accuracy: 0.9732 - val_loss: 1.2731 - val_categorical_accuracy: 0.7933\n",
      "Epoch 32/50\n",
      "9034/9034 [==============================] - 16s 2ms/step - loss: 0.1224 - categorical_accuracy: 0.9752 - val_loss: 1.3257 - val_categorical_accuracy: 0.7902\n",
      "Epoch 33/50\n",
      "9034/9034 [==============================] - 16s 2ms/step - loss: 0.1153 - categorical_accuracy: 0.9763 - val_loss: 1.3237 - val_categorical_accuracy: 0.7857\n",
      "Epoch 34/50\n",
      "9034/9034 [==============================] - 16s 2ms/step - loss: 0.1134 - categorical_accuracy: 0.9762 - val_loss: 1.2582 - val_categorical_accuracy: 0.7897\n",
      "Epoch 35/50\n",
      "9034/9034 [==============================] - 16s 2ms/step - loss: 0.1131 - categorical_accuracy: 0.9766 - val_loss: 1.3385 - val_categorical_accuracy: 0.7924\n",
      "Epoch 36/50\n",
      "9034/9034 [==============================] - 16s 2ms/step - loss: 0.1080 - categorical_accuracy: 0.9782 - val_loss: 1.3096 - val_categorical_accuracy: 0.7937\n",
      "Epoch 37/50\n",
      "9034/9034 [==============================] - 16s 2ms/step - loss: 7.5174 - categorical_accuracy: 0.5265 - val_loss: 11.7593 - val_categorical_accuracy: 0.2390\n",
      "Epoch 38/50\n",
      "9034/9034 [==============================] - 16s 2ms/step - loss: 0.3006 - categorical_accuracy: 0.9587 - val_loss: 1.4827 - val_categorical_accuracy: 0.7813\n",
      "Epoch 39/50\n",
      "9034/9034 [==============================] - 16s 2ms/step - loss: 0.1260 - categorical_accuracy: 0.9737 - val_loss: 1.4017 - val_categorical_accuracy: 0.7933\n",
      "Epoch 40/50\n",
      "9034/9034 [==============================] - 16s 2ms/step - loss: 0.1535 - categorical_accuracy: 0.9671 - val_loss: 1.5759 - val_categorical_accuracy: 0.7703\n",
      "Epoch 41/50\n",
      "9034/9034 [==============================] - 15s 2ms/step - loss: 0.1308 - categorical_accuracy: 0.9727 - val_loss: 1.4392 - val_categorical_accuracy: 0.7866\n",
      "Epoch 42/50\n",
      "9034/9034 [==============================] - 16s 2ms/step - loss: 0.1229 - categorical_accuracy: 0.9755 - val_loss: 1.4564 - val_categorical_accuracy: 0.7844\n",
      "Epoch 43/50\n",
      "9034/9034 [==============================] - 15s 2ms/step - loss: 0.0978 - categorical_accuracy: 0.9792 - val_loss: 1.4311 - val_categorical_accuracy: 0.7906\n",
      "Epoch 44/50\n",
      "9034/9034 [==============================] - 16s 2ms/step - loss: 0.0920 - categorical_accuracy: 0.9796 - val_loss: 1.4108 - val_categorical_accuracy: 0.7893\n",
      "Epoch 45/50\n",
      "9034/9034 [==============================] - 16s 2ms/step - loss: 0.0948 - categorical_accuracy: 0.9815 - val_loss: 1.5597 - val_categorical_accuracy: 0.7831\n",
      "Epoch 46/50\n",
      "9034/9034 [==============================] - 15s 2ms/step - loss: 0.0981 - categorical_accuracy: 0.9811 - val_loss: 1.5501 - val_categorical_accuracy: 0.7866\n",
      "Epoch 47/50\n",
      "9034/9034 [==============================] - 16s 2ms/step - loss: 0.1283 - categorical_accuracy: 0.9743 - val_loss: 1.5946 - val_categorical_accuracy: 0.7831\n",
      "Epoch 48/50\n",
      "9034/9034 [==============================] - 15s 2ms/step - loss: 0.1130 - categorical_accuracy: 0.9759 - val_loss: 1.4055 - val_categorical_accuracy: 0.8043\n",
      "Epoch 49/50\n",
      "9034/9034 [==============================] - 15s 2ms/step - loss: 0.0931 - categorical_accuracy: 0.9800 - val_loss: 1.4816 - val_categorical_accuracy: 0.7928\n",
      "Epoch 50/50\n",
      "9034/9034 [==============================] - 15s 2ms/step - loss: 0.0913 - categorical_accuracy: 0.9800 - val_loss: 1.3897 - val_categorical_accuracy: 0.7959\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(trainX,\n",
    "                    trainY,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(validateX, validateY),\n",
    "                    callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pandas\n",
    "# To evaluate the model we want the test reader to load the test set since it was in a separate file.\n",
    "# but we want to use the original vocabulary to define the sequences.\n",
    "test_reader = TextReader.TextReader(os.path.join(basedir, 'news20'),\n",
    "                                    basedir)\n",
    "\n",
    "testvocab = []\n",
    "test_words = []\n",
    "test_classes = []\n",
    "test_targets = None\n",
    "test_sequences = None\n",
    "\n",
    "test_path = os.path.join(basedir, 'news20')\n",
    "test_path = os.path.join(test_path, 'test_data.pickle')\n",
    "\n",
    "if os.path.exists(test_path):\n",
    "    with open(test_path, 'rb') as fin:\n",
    "        all_data = pickle.load(fin)\n",
    "        testvocab = all_data['test_vocab']\n",
    "        test_words = all_data['test_words']\n",
    "        test_classes = all_data['test_classes']\n",
    "        test_targets = all_data['test_targets']\n",
    "        test_sequences = all_data['test_sequences']\n",
    "else:\n",
    "    testvocab, test_words, test_classes = test_reader.read_labeled_documents('20ng-test-all-terms.txt')\n",
    "\n",
    "    # get the test targets\n",
    "    test_targets = test_reader.one_hot_encode_classes(test_classes)\n",
    "    # get the test sequences but use the indexes in the vocabulary we trained on.\n",
    "    # words in the test set not in the original vocab are substituted with '<UNKNOWN>'\n",
    "    test_sequences = test_reader.make_index_sequences(vocab, test_words)\n",
    "    # we need to set the max width of sequences to equal the maximum width of the\n",
    "    # training data.\n",
    "    test_width = test_sequences.shape[1]\n",
    "    if test_width > max_sequence_length:\n",
    "        delta = test_width - max_sequence_length\n",
    "        test_sequences = test_sequences.iloc[:, delta:]\n",
    "    elif test_width < max_sequence_length:\n",
    "        # otherwise we need to pad the sequences so they are the same length.\n",
    "        padding = vocab.index('<NA>')\n",
    "        delta = max_sequence_length - test_width\n",
    "        rows = test_sequences.shape[0]\n",
    "        pad_cells = np.tile(padding, [rows, max_sequence_length])\n",
    "        endcol = max_sequence_length -  1\n",
    "        for i in range(0, rows):\n",
    "            pad_cells[i,delta:max_sequence_length] = test_sequences.iloc[i,:]\n",
    "        test_sequences = pandas.DataFrame(pad_cells)\n",
    "        \n",
    "    all_data = {\n",
    "        'test_vocab': testvocab,\n",
    "        'test_words': test_words,\n",
    "        'test_classes': test_classes,\n",
    "        'test_targets': test_targets,\n",
    "        'test_sequences': test_sequences\n",
    "    }\n",
    "    with open(test_path, 'wb') as fout:\n",
    "        pickle.dump(all_data, fout)\n",
    "        \n",
    "test_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>6967</th>\n",
       "      <th>6968</th>\n",
       "      <th>6969</th>\n",
       "      <th>6970</th>\n",
       "      <th>6971</th>\n",
       "      <th>6972</th>\n",
       "      <th>6973</th>\n",
       "      <th>6974</th>\n",
       "      <th>6975</th>\n",
       "      <th>6976</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>62048</td>\n",
       "      <td>70067</td>\n",
       "      <td>16448</td>\n",
       "      <td>31963</td>\n",
       "      <td>44962</td>\n",
       "      <td>13338</td>\n",
       "      <td>15288</td>\n",
       "      <td>34014</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>68051</td>\n",
       "      <td>36886</td>\n",
       "      <td>25999</td>\n",
       "      <td>39955</td>\n",
       "      <td>2045</td>\n",
       "      <td>7938</td>\n",
       "      <td>34145</td>\n",
       "      <td>18131</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>69973</td>\n",
       "      <td>64717</td>\n",
       "      <td>66057</td>\n",
       "      <td>52587</td>\n",
       "      <td>41767</td>\n",
       "      <td>29107</td>\n",
       "      <td>529</td>\n",
       "      <td>39186</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>58569</td>\n",
       "      <td>52587</td>\n",
       "      <td>35099</td>\n",
       "      <td>36296</td>\n",
       "      <td>353</td>\n",
       "      <td>18015</td>\n",
       "      <td>63022</td>\n",
       "      <td>35099</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 6977 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4     5     6     7     8     9     ...    6967  \\\n",
       "0     1     1     1     1     1     1     1     1     1     1  ...   62048   \n",
       "1     1     1     1     1     1     1     1     1     1     1  ...   68051   \n",
       "2     1     1     1     1     1     1     1     1     1     1  ...   69973   \n",
       "3     1     1     1     1     1     1     1     1     1     1  ...   58569   \n",
       "\n",
       "    6968   6969   6970   6971   6972   6973   6974  6975  6976  \n",
       "0  70067  16448  31963  44962  13338  15288  34014     0     3  \n",
       "1  36886  25999  39955   2045   7938  34145  18131     0     3  \n",
       "2  64717  66057  52587  41767  29107    529  39186     0     3  \n",
       "3  52587  35099  36296    353  18015  63022  35099     0     3  \n",
       "\n",
       "[4 rows x 6977 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sequences.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_targets.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_data = {\n",
    "        'test_vocab': testvocab,\n",
    "        'test_words': test_words,\n",
    "        'test_classes': test_classes,\n",
    "        'test_targets': test_targets,\n",
    "        'test_sequences': test_sequences\n",
    "}\n",
    "with open(test_path, 'wb') as fout:\n",
    "    pickle.dump(all_data, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7528/7528 [==============================] - 5s 599us/step\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_sequences, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.3952712082214, 0.6859723698193412)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
